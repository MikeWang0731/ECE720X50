{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "This notebook provides a skeleton for the code you need to submit for Question 1. Using this notebook should guide you through the question and ensure that you're staying on the right track, but it's not necessary that use it to complete the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start with some imports. These are all the imports that are needed to complete this question. If you prefer to use different libraries/methods, feel free to add to or replace these imports as you see fit. It's not required that you use these specific packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the training data and see what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('wine_training.csv')\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1280 samples with 11 continuous features ('fixed acidity' through to 'alcohol') which you will use to predict the 'quality'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing data analogously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv('wine_testing.csv')\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 320 testing samples with the same 11 features. You will use these features to predict the wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, convert the data to a numpy array and separate the training inputs from the training outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_numpy = training_data.to_numpy()\n",
    "testing_numpy = testing_data.to_numpy()\n",
    "\n",
    "X = training_numpy[:,0:-1]\n",
    "y = training_numpy[:,-1]\n",
    "X_test = testing_numpy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a good time to get a feel for the data. The best way to do this is performs some exploratory data analysis. This involves calulating various statistics on the data and generating plots with the aim of learning as much about the data as possible. The idea is that if you are familiar with the data, it will be easier to analyze results, troubleshoot errors, choose model parameters, etc. Start by calculating the mean and standard deviation of each feature and move on to histograms, correlations, and whatever else you would like to know about the data. Some question you may want to answer before moving forward:\n",
    "* are there any missing values?\n",
    "* are there any outliers which could hurt our model?\n",
    "* can we remove any features becuse they don't add any information?\n",
    "\n",
    "In general, there is no one right way to do exploratory data analysis. The point is to make sure you are familiar with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Do some exploratory data analysis to get familiar with the data'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done some exploratory data analysis, it may be a good idea to preprocess the data. This can be done in many ways and, again, there is no one way which is right. As a minimum, it's a good idea to standardize the features. You can do this using the StandardScaler class in sklearn.\n",
    "\n",
    "A more powerful and sophisticated method is Principal Component Analysis (PCA) which is discussed in the 09-Unsupervised-Learning-Part2 notebook on eclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Preprocess the dataset using a method you deem appropriate given the data'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Regression\n",
    "\n",
    "Now that the input features have been preprocessed, you can build the feedforward neural network model for regression. There are several ways to do this in Keras but the easiest way is to use the Sequential class with Dense layers. Below is a skeleton of the code to help you get started. Experiment with different number of layers, layer sizes, activation functions, etc. Remember, if you're ever unsure about how a piece of code works, the documentation is your best friend: https://keras.io/api/\n",
    "\n",
    "One important question you should answer before building the model is how many outputs should the neural network have given that the task is regression. If the output of the network does not match the training outputs, you will run into errors.\n",
    "\n",
    "Using the Sequential API is sufficient for his question but if you're interested in creating more complicated models, look into Keras' Functional API. The rest of this notebook is written using the Sequential API.\n",
    "\n",
    "Note: this code is not set up for automated hyperparameter exploration. If you wish to do this, you will need to make some changes such as wrapping model building, training, and evaluation inside for loops which search the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense('''TODO: Implement the first layer in your neural network'''),\n",
    "    Dense('''TODO: Implement the second layer in your neural network'''),\n",
    "    '''TODO: Do you want a deeper network? '''\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is built, you have to compile it. This step is important becasue it defines how the model will learn its parameters and make predictions. As before, you are given a code skeleton to fill out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='''TODO: Which optimizer should you use?''',\n",
    "  loss='''TODO: Which loss function should you use?''',\n",
    "  metrics='''TODO: Which metrics should you use? Hint: read the assignment description :)''',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been built and compiled, it is ready to train. To ensure that the model does not overfit to the training data, it's best to perform 10-fold cross-validation. This part has been set up for you using the KFold class.\n",
    "\n",
    "Two methods which are useful for this step are model.fit(...) and model.evaluate(...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "for fold_train_idx, fold_val_idx in kf.split(X):\n",
    "    '''TODO: Train and evaluate the model on the current fold'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can use it to predict the quality of the wine for the testing data. Note that any preprocessing steps you applied to the training data, you will also have to apply to the testing data. Consider using model.predict(...) for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    '''TODO: Use the trained model to predict the wine quality on the testing data'''\n",
    ")\n",
    "'''Save your predictions to a csv file'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classification\n",
    "\n",
    "The first step in classification is to divide the training data into its three classes. Recall that the classes are:\n",
    "\n",
    "* wine belongs to class 'Good' if quality >= 7\n",
    "* wine belongs to class 'Average' if 7 > quality >= 5\n",
    "* wine belongs to class 'Bad' if 5 > quality\n",
    "\n",
    "\n",
    "It's imporant to remember that the task of regression is not the same as classification. As such, using three ordinal classes such as ['Bad' = 1, 'Average' = 2, 'Good' = 3] is not appropriate. To understand the intuition behind this, ask yourself what value should the model predict if it assigns a 50% probability that a wine is 'Bad', a 0% probability that a wine is 'Average', and a 50% probability that a wine is 'Good'. Answering this question is a first step in developing an encoding scheme for the classes.\n",
    "\n",
    "Hint: look at the imports that are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Find an encoding scheme which you can use for the three classes and map each of the wines in the training data to its appropriate class'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having encoded the outputs for the training data, you can now build and compile the model. This can be done the same as before but pay close attention to the model outputs, the activation function in the final layer, and the loss function used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Build and compile the model for classification'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model using 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Train and evaluate the model using 10-fold cross-validation'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can use it to predict the class of the wine in the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Use the trained model to predict the wine class on the testing data. Save your predictions to a csv file'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
